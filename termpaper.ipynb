{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d772503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3844ef0",
   "metadata": {},
   "source": [
    "Part 1\n",
    "\n",
    "1. Write code to read all SCE files and merge them into a single DataFrame.\n",
    "2. Report the following statistics to get an idea of the sample size:\n",
    "1. Number of unique individuals in the dataset.\n",
    "2\n",
    "2. Number of observations (rows) in the dataset.\n",
    "3. Number of unique survey waves.\n",
    "4. The first and last dates observed in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98bd260",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "136666b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>wid</th>\n",
       "      <th>date</th>\n",
       "      <th>weight</th>\n",
       "      <th>female</th>\n",
       "      <th>educ</th>\n",
       "      <th>age</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>black</th>\n",
       "      <th>couple</th>\n",
       "      <th>...</th>\n",
       "      <th>num_lit_q3</th>\n",
       "      <th>num_lit_q3_correct</th>\n",
       "      <th>num_lit_q5</th>\n",
       "      <th>num_lit_q5_correct</th>\n",
       "      <th>num_lit_q6</th>\n",
       "      <th>num_lit_q6_correct</th>\n",
       "      <th>num_lit_q8</th>\n",
       "      <th>num_lit_q8_correct</th>\n",
       "      <th>num_lit_q9</th>\n",
       "      <th>num_lit_q9_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70024705</td>\n",
       "      <td>201404</td>\n",
       "      <td>2014-04-03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70024720</td>\n",
       "      <td>201404</td>\n",
       "      <td>2014-04-02</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70024744</td>\n",
       "      <td>201404</td>\n",
       "      <td>2014-04-02</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70024764</td>\n",
       "      <td>201404</td>\n",
       "      <td>2014-04-08</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70024781</td>\n",
       "      <td>201404</td>\n",
       "      <td>2014-04-10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176096</th>\n",
       "      <td>75023169</td>\n",
       "      <td>202409</td>\n",
       "      <td>2024-09-19</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176097</th>\n",
       "      <td>75023181</td>\n",
       "      <td>202409</td>\n",
       "      <td>2024-09-07</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176098</th>\n",
       "      <td>75023187</td>\n",
       "      <td>202409</td>\n",
       "      <td>2024-09-19</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176099</th>\n",
       "      <td>75023188</td>\n",
       "      <td>202409</td>\n",
       "      <td>2024-09-03</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176100</th>\n",
       "      <td>75023201</td>\n",
       "      <td>202409</td>\n",
       "      <td>2024-09-20</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176101 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          userid     wid       date  weight  female  educ   age  hispanic  \\\n",
       "0       70024705  201404 2014-04-03     1.0     0.0   3.0  23.0       0.0   \n",
       "1       70024720  201404 2014-04-02     0.5     0.0   4.0  65.0       0.0   \n",
       "2       70024744  201404 2014-04-02     0.6     0.0   4.0  52.0       0.0   \n",
       "3       70024764  201404 2014-04-08     0.4     1.0   4.0  46.0       0.0   \n",
       "4       70024781  201404 2014-04-10     0.8     1.0   3.0  61.0       0.0   \n",
       "...          ...     ...        ...     ...     ...   ...   ...       ...   \n",
       "176096  75023169  202409 2024-09-19     2.5     1.0   1.0  23.0       0.0   \n",
       "176097  75023181  202409 2024-09-07     1.1     1.0   3.0  53.0       1.0   \n",
       "176098  75023187  202409 2024-09-19     0.8     0.0   4.0  51.0       0.0   \n",
       "176099  75023188  202409 2024-09-03     0.7     1.0   4.0  53.0       0.0   \n",
       "176100  75023201  202409 2024-09-20     2.3     1.0   2.0  43.0       0.0   \n",
       "\n",
       "        black  couple  ...  num_lit_q3  num_lit_q3_correct  num_lit_q5  \\\n",
       "0         0.0     NaN  ...         NaN                 NaN         NaN   \n",
       "1         0.0     NaN  ...         NaN                 NaN         NaN   \n",
       "2         0.0     NaN  ...         NaN                 NaN         NaN   \n",
       "3         0.0     NaN  ...         NaN                 NaN         NaN   \n",
       "4         0.0     NaN  ...         NaN                 NaN         NaN   \n",
       "...       ...     ...  ...         ...                 ...         ...   \n",
       "176096    1.0     0.0  ...         2.0                 0.0        20.0   \n",
       "176097    1.0     1.0  ...        10.0                 1.0      1000.0   \n",
       "176098    1.0     0.0  ...       100.0                 0.0       100.0   \n",
       "176099    0.0     0.0  ...        10.0                 1.0       100.0   \n",
       "176100    0.0     1.0  ...        10.0                 1.0       100.0   \n",
       "\n",
       "        num_lit_q5_correct  num_lit_q6  num_lit_q6_correct  num_lit_q8  \\\n",
       "0                      NaN         NaN                 NaN         NaN   \n",
       "1                      NaN         NaN                 NaN         NaN   \n",
       "2                      NaN         NaN                 NaN         NaN   \n",
       "3                      NaN         NaN                 NaN         NaN   \n",
       "4                      NaN         NaN                 NaN         NaN   \n",
       "...                    ...         ...                 ...         ...   \n",
       "176096                 0.0         1.0                 0.0         2.0   \n",
       "176097                 0.0         4.0                 0.0         1.0   \n",
       "176098                 1.0         5.0                 1.0         3.0   \n",
       "176099                 1.0         5.0                 1.0         3.0   \n",
       "176100                 1.0         5.0                 1.0         1.0   \n",
       "\n",
       "        num_lit_q8_correct  num_lit_q9  num_lit_q9_correct  \n",
       "0                      NaN         NaN                 NaN  \n",
       "1                      NaN         NaN                 NaN  \n",
       "2                      NaN         NaN                 NaN  \n",
       "3                      NaN         NaN                 NaN  \n",
       "4                      NaN         NaN                 NaN  \n",
       "...                    ...         ...                 ...  \n",
       "176096                 0.0         2.0                 1.0  \n",
       "176097                 0.0         1.0                 0.0  \n",
       "176098                 1.0         2.0                 1.0  \n",
       "176099                 1.0         2.0                 1.0  \n",
       "176100                 0.0         2.0                 1.0  \n",
       "\n",
       "[176101 rows x 29 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in files using glob library and for loop\n",
    "files = glob.glob('../TECH2-GROUP-TERM-PAPER/data/*.csv')\n",
    "dfs = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, sep= ';', parse_dates = ['date'])\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs, ignore_index = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa40edaf",
   "metadata": {},
   "source": [
    "Part 2\n",
    "\n",
    "1. The numeracy questions are only asked the first time a respondent enters the survey and are\n",
    "missing in subsequent waves. For each individual, fill in the missing numeracy variables\n",
    "(num_lit_X_correct where X is a number) using the values from the first observation.\n",
    "2. Drop all observations (rows) with missing values for the following variables:\n",
    "1. Demographic information (gender, age, education)\n",
    "2. The three expectations questions about inflation, house price changes, and the stock market\n",
    "3. The seven numeracy questions (after you have forward-filled nonmissing values in step 1!)\n",
    "Report the number of observations dropped at each step.\n",
    "3. Drop outliers (implausibly small or large values). For each expectations response:\n",
    "1. Compute the 0.1th percentile (0.001 quantile) and drop observations below this value.\n",
    "2. Compute the 99.9th percentile (0.999 quantile) and drop observations above this value.\n",
    "Report the number of observations dropped at each step.\n",
    "4. Create a new column college equal to 1 if an individual has at least a bachelor’s degree, and 0\n",
    "otherwise.\n",
    "5. For each individual, compute the total number of correct numeracy responses and report the\n",
    "fraction of individuals with 0, 1, . . . , 7 correct responses (e.g., 36.2% of individuals got all 7 right).\n",
    "Create a new column num_lit_high (“high numerical literacy”) equal to 1 if an individual had\n",
    "more correct responses than the median, and 0 otherwise.\n",
    "6. Report the same sample statistics as in Part 1 for the final data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feea97a",
   "metadata": {},
   "source": [
    "Part 3\n",
    "\n",
    "With the processed data set in hand, you now turn to answering the governor’s first question: how do\n",
    "expectations differ by gender, education, and numeracy?\n",
    "For this part, perform the same analysis three times, each time splitting the sample by a different\n",
    "variable:\n",
    "1. Males vs. females\n",
    "2. Non-college vs. college\n",
    "3. Low vs. high numeracy\n",
    "\n",
    "For each part,\n",
    "1. Compute the average for each expectations variable (inflation, house prices, stock market) sepa-\n",
    "rately for each group (males and females, non-college and college, etc.)\n",
    "2. Create a figure with three panels (one per expectations variable) which depicts these group\n",
    "averages as bar charts.\n",
    "3\n",
    "The complete analysis should produce three figures, each with three panels, each panel containing two\n",
    "bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57b3b415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average answer for INFLATION from the Male group is 4.9.\n",
      "The average answer for INFLATION from the Female group is 7.4.\n",
      "\n",
      "The average answer for HOUSE PRICE CHANGE from the Male group is 4.6.\n",
      "The average answer for HOUSE PRICE CHANGE from the Female group is 6.5.\n",
      "\n",
      "The average answer for PROB STOCKS UP from the Male group is 45.9.\n",
      "The average answer for PROB STOCKS UP from the Female group is 38.0.\n"
     ]
    }
   ],
   "source": [
    "# 1. average for each expectations variable\n",
    "# Males Vs. Females\n",
    "\n",
    "# Create a function to calculate and print the average variables \n",
    "df_gender = df.groupby(['female'])\n",
    "def df_gender_calculation(variable):\n",
    "    variable_neat = variable.replace('_', ' ').upper()\n",
    "    variable_average = round(df_gender[variable].mean(),1)\n",
    "    variable_average = variable_average.rename({0.0: 'Male', 1.0: 'Female'})\n",
    "    variabledc = variable_average.to_dict()\n",
    "    for key, value in variabledc.items(): \n",
    "        print(f'The average answer for {variable_neat} from the {key} group is {value}.')\n",
    "\n",
    "df_gender_calculation(variable = 'inflation')\n",
    "print('')\n",
    "df_gender_calculation(variable = 'house_price_change')\n",
    "print('')\n",
    "df_gender_calculation(variable = 'prob_stocks_up')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f709a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average answer for INFLATION from the college educated group is 5.5.\n",
      "The average answer for INFLATION from the non college educated group is 16.3.\n",
      "\n",
      "The average answer for HOUSE PRICE CHANGE from the college educated group is 5.1.\n",
      "The average answer for HOUSE PRICE CHANGE from the non college educated group is 8.9.\n",
      "\n",
      "The average answer for PROB STOCKS UP from the college educated group is 43.0.\n",
      "The average answer for PROB STOCKS UP from the non college educated group is 34.3.\n"
     ]
    }
   ],
   "source": [
    "# non-college and college\n",
    "df_college = df[df['educ']>2]\n",
    "df_non_college= df[df['educ']<2]\n",
    "\n",
    "def df_college_calculation(variable):\n",
    "    variable_neat = variable.replace('_', ' ').upper()\n",
    "    df_educ ={}\n",
    "    variable_averagec = round(df_college[variable].mean(), 1)\n",
    "    variable_averagenc = round(df_non_college[variable].mean(),1)\n",
    "    df_educ['college educated'] = variable_averagec\n",
    "    df_educ['non college educated'] = variable_averagenc\n",
    "    for key, value in df_educ.items():\n",
    "        print(f'The average answer for {variable_neat} from the {key} group is {value}.')\n",
    "\n",
    "df_college_calculation(variable = 'inflation')\n",
    "print('')\n",
    "df_college_calculation(variable = 'house_price_change')\n",
    "print('')\n",
    "df_college_calculation(variable = 'prob_stocks_up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "564be27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average answer for INFLATION from the high numeracy group is 8.2.\n",
      "The average answer for INFLATION from the low numeracy group is 14.1.\n",
      " \n",
      "The average answer for HOUSE PRICE CHANGE from the high numeracy group is 6.2.\n",
      "The average answer for HOUSE PRICE CHANGE from the low numeracy group is 12.2.\n",
      "\n",
      "The average answer for PROB STOCKS UP from the high numeracy group is 47.8.\n",
      "The average answer for PROB STOCKS UP from the low numeracy group is 41.5.\n"
     ]
    }
   ],
   "source": [
    "#low vs high numeracy\n",
    "# calculate average numeracy for each person\n",
    "columns= ['num_lit_q1_correct',\n",
    " 'num_lit_q2_correct',\n",
    " 'num_lit_q3_correct',\n",
    " 'num_lit_q5_correct',\n",
    " 'num_lit_q6_correct',\n",
    " 'num_lit_q8_correct',\n",
    " 'num_lit_q9_correct']\n",
    "df['Numeracy'] = df[columns].mean(axis = 1)\n",
    "\n",
    "#divide between high and low numeracy\n",
    "df_high = df[df['Numeracy']>0.5]\n",
    "df_low = df[df['Numeracy']<0.5]\n",
    "\n",
    "#function to calculate average for each variable\n",
    "def df_numeracy_calculation(variable):\n",
    "    variable_neat = variable.replace('_', ' ').upper()\n",
    "    df_num ={}\n",
    "    variable_averageh = round(df_high[variable].mean(), 1)\n",
    "    variable_averagel = round(df_low[variable].mean(),1)\n",
    "    df_num['high numeracy'] = variable_averageh\n",
    "    df_num['low numeracy'] = variable_averagel\n",
    "    for key, value in df_num.items():\n",
    "        print(f'The average answer for {variable_neat} from the {key} group is {value}.')\n",
    "\n",
    "df_numeracy_calculation(variable = 'inflation')\n",
    "print(' ')\n",
    "df_numeracy_calculation(variable = 'house_price_change')\n",
    "print('')\n",
    "df_numeracy_calculation(variable = 'prob_stocks_up')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc21f9a8",
   "metadata": {},
   "source": [
    "Part 4\n",
    "\n",
    "To answer the governor’s second set of questions, you need to investigate how average expectations\n",
    "evolved over time for the period of 2015-2024 covered by the sample.\n",
    "For each grouping variable (female, college, num_lit_high):\n",
    "1. Collapse the data to monthly averages for each of the expectation variables (inflation, house prices,\n",
    "stock market) for each group (e.g., for males vs. females).\n",
    "2. Create a figure with three vertically stacked panels (one panel per expectation variable). Each\n",
    "panel should show the group time series (two series per panel, e.g., males vs. females) with time\n",
    "on the x-axis.\n",
    "3. The governor wants to know how expectations reacted to important geopolitical events. Add\n",
    "vertical lines and annotations to each panel indicating the following events:\n",
    "• Trump elected US president for the first time (November 8, 2016)\n",
    "• COVID-19 pandemic goes global (February 1, 2020)\n",
    "• Biden elected US president (November 3, 2020)\n",
    "• Russia’s full-scale invasion of Ukraine (February 24, 2022)\n",
    "• Nobel Prize in Literature awarded to Jon Fosse (October 3, 2023)\n",
    "• Trump elected US president for the second time (November 5, 2024)\n",
    "Which events had sizeable effects on expectations? Can you detect differences in how different\n",
    "groups adjusted their expectations?\n",
    "The complete analysis should produce three figures, each with three panels, each panel containing two\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095df410",
   "metadata": {},
   "source": [
    "Part 5\n",
    "\n",
    "Finally, the governor is interested in whether people’s expectations indicate realized future inflation or\n",
    "simply reflect past inflation experienced when answering the survey.\n",
    "To answer this question, you first need to obtain data on realized inflation. A colleague has already\n",
    "downloaded data on the level of the Consumer Price Index (CPI) from FRED and stored it as a CSV file\n",
    "in the data/ folder.\n",
    "1. First, compare expectations to realized future inflation:\n",
    "1. Using this monthly CPI data, compute the realized inflation over the next 12 months; i.e., for\n",
    "each month t compute the forward-looking annual inflation as\n",
    "In f lationt = CPIt+12 −CPIt\n",
    "CPIt\n",
    "×100\n",
    "2. Merge this inflation measure with the monthly averages by gender from Part 4. Specifically,\n",
    "match the average expected inflation by gender i in month t from the SCE, Ex pIn f lationit,\n",
    "with the forward-looking inflation measure In f lationt from the CPI data.\n",
    "3. Create a figure with two panels (one per gender), each showing a scatter plot of realized\n",
    "future inflation (y-axis) versus average expected inflation by gender.\n",
    "Compute the correlation between expected and realized inflation for each gender and add\n",
    "the correlation coefficient as text to the corresponding panel.\n",
    "4\n",
    "2. Repeat steps 1–3, but instead of forward-looking inflation, compute realized inflation over the past\n",
    "12 months:\n",
    "In f lationt = CPIt −CPIt−12\n",
    "CPIt−12\n",
    "×100\n",
    "Do you find differences in these correlation coefficients? What do these results say about how individuals\n",
    "form beliefs about inflation? Are there notable gender differences?\n",
    "Note: In this part, use monthly data only; individual-level SCE data is not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e290b49d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TECH2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
